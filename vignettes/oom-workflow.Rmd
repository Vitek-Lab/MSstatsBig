---
title: "Pre-processing out-of-memory data with MSstatsBig"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Pre-processing out-of-memory data with MSstatsBig}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Processing out-of-memory data with MSstatsBig

Converting a Spectrounat file that does not fit in memory with MSstats requires two steps.
First, `cleanBigSpectronautArrow` function applies filters and column selection to reduce the size of input file.
Data can be filtered with `EG.Identified` column (`filter_by_excluded` function) and both protein-, and psm-level q-values (`filter_by_qvalue` and `qvalue_cutoff` functions).
Additionally, source column for intensity can be selected. By default, `F.PeakArea` column is used.
This does not return any R output. Instead, it creates a csv file with reduced data set.
If such data set fits in memory and does not require special treatment for sparse labels, it can be processed with standard `MSstatsConvert` functions.
If sparse label processing is needed or the data set doesn't fit in memory, `BigSpectronauttoMSstatsFormat` function can be used to convert the reduced data set into full `MSstats` format by aggregating multiple observations per featuring and run, removing features with a low number of observations, and removing shared peptides.
Again, this function does not return any R output. Instead, it creates a file given by its second parameter.

```{r }
library(MSstatsBig)

fg_input = system.file("fgexample.csv", package = "MSstatsBig")

fg_input = system.file("extdata", "fgexample.csv", package = "MSstatsBig")

package_folder = system.file(package = "MSstatsBig")
list.files(package_folder)

fg_arr = BigFragPipetoMSstatsFormat(
  fg_input,
  "fg_processed.csv", 
  backend = "arrow",
  max_feature_count = 10)
dplyr::collect(fg_arr)
```

Session info:

```{r }
sessionInfo()
```
